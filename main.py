import requests
from bs4 import BeautifulSoup


def get_article_links(section_url, max_articles=5):
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(section_url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    links = []
    for tag in soup.select("a[data-link-name='article']"):
        href = tag.get("href")
        if (
            href
            and href.startswith("https://www.theguardian.com/")
            and href not in links
        ):
            links.append(href)
        if len(links) >= max_articles:
            break
    return links


def get_article_content(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    try:
        title = soup.find("h1").get_text(strip=True)
    except:
        title = "(ì œëª© ì—†ìŒ)"

    # ë³¸ë¬¸ì´ ë“¤ì–´ìˆëŠ” ì‹¤ì œ selector ìˆ˜ì •
    paragraphs = soup.select("div[data-gu-name='body'] p") or soup.select(
        "div.article-body-commercial-selector p"
    )
    content = "\n".join(p.get_text(strip=True) for p in paragraphs)
    return {"title": title, "content": content, "url": url}


def main():
    tech_links = get_article_links("https://www.theguardian.com/uk/technology")
    sci_links = get_article_links("https://www.theguardian.com/science")
    all_links = tech_links + sci_links

    all_articles = []

    if not all_links:
        print("âš ï¸ ê¸°ì‚¬ ë§í¬ë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    for link in all_links:
        article = get_article_content(link)
        all_articles.append(article)

        print(f"\n[ì œëª©] {article['title']}")
        print(f"[ë§í¬] {article['url']}")
        print(f"[ë³¸ë¬¸ ìš”ì•½] {article['content'][:200]}...")
        print("-" * 80)

    # âœ… ì—¬ê¸°ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    with open("guardian_articles.txt", "w", encoding="utf-8") as f:
        for article in all_articles:
            f.write(f"ì œëª©: {article['title']}\n")
            f.write(f"ë§í¬: {article['url']}\n")
            f.write(f"ë³¸ë¬¸:\n{article['content']}\n")
            f.write("="*100 + "\n")

    print("\nğŸ“ ê¸°ì‚¬ë“¤ì´ 'guardian_articles.txt'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
